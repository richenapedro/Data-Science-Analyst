{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Alcohol Consumption Analysiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This Jupyter Notebook aims to analyze the factors influencing alcohol consumption among secondary school students. By exploring the dataset, we will gain insights into how various social, educational, and personal factors affect students' alcohol consumption patterns and academic performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Package Installation](#package-installation)\n",
    "2. [Library Imports](#library-imports)\n",
    "3. [Data Loading](#data-loading)\n",
    "4. [Initial Data Exploration](#initial-data-exploration)\n",
    "5. [Data Visualization](#data-visualization)\n",
    "6. [Correlation Analysis](#correlation-analysis)\n",
    "7. [Data Preparation](#data-preparation)\n",
    "8. [Modeling](#modeling)\n",
    "9. [Model Evaluation](#model-evaluation)\n",
    "10. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation <a name=\"package-installation\"></a>\n",
    "In this cell, we will install the necessary Python packages required for our data analysis project. This step ensures that all the libraries needed for data manipulation, visualization, and machine learning are available in our environment. \n",
    "\n",
    "We will use the `pip` command to install the following libraries:\n",
    "\n",
    "- **NumPy**: A library for numerical computations and handling arrays.\n",
    "- **Pandas**: A powerful data manipulation and analysis library, particularly useful for working with structured data.\n",
    "- **Matplotlib**: A plotting library for creating static, animated, and interactive visualizations in Python.\n",
    "- **Seaborn**: A statistical data visualization library based on Matplotlib that provides a high-level interface for drawing attractive graphics.\n",
    "- **Scikit-learn**: A machine learning library that provides simple and efficient tools for data mining and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following libraries are missing: scikit-learn\n",
      "Starting installation...\n",
      "Installing scikit-learn...\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\richena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\richena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\richena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\richena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\richena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "scikit-learn installed successfully.\n",
      "Error: scikit-learn was not installed correctly.\n",
      "Some libraries were not installed correctly.\n"
     ]
    }
   ],
   "source": [
    "# List of required libraries\n",
    "required_libraries = [\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "def install(package):\n",
    "    \"\"\"Install the package using pip in a Jupyter Notebook.\"\"\"\n",
    "    print(f\"Installing {package}...\")\n",
    "    # Use the Jupyter magic command for installation\n",
    "    get_ipython().system(f'pip install {package}')\n",
    "\n",
    "def check_libraries(libraries):\n",
    "    \"\"\"Check if the libraries are installed and install them if necessary.\"\"\"\n",
    "    missing_libraries = []\n",
    "\n",
    "    for library in libraries:\n",
    "        try:\n",
    "            __import__(library)\n",
    "        except ImportError:\n",
    "            missing_libraries.append(library)\n",
    "        except Exception as e:\n",
    "            # Captura outros erros que podem ocorrer durante a importação\n",
    "            print(f\"Error importing {library}: {e}\")\n",
    "            missing_libraries.append(library)\n",
    "\n",
    "    if missing_libraries:\n",
    "        print(f\"The following libraries are missing: {', '.join(missing_libraries)}\")\n",
    "        print(\"Starting installation...\")\n",
    "\n",
    "        installation_success = True  # Flag to track installation success\n",
    "\n",
    "        for library in missing_libraries:\n",
    "            try:\n",
    "                install(library)\n",
    "                print(f\"{library} installed successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to install {library}: {e}\")\n",
    "                installation_success = False  # Mark as failed if there was an error\n",
    "\n",
    "        # Check again if the libraries were installed\n",
    "        for library in missing_libraries:\n",
    "            try:\n",
    "                __import__(library)\n",
    "            except ImportError:\n",
    "                print(f\"Error: {library} was not installed correctly.\")\n",
    "                installation_success = False  # Mark as failed if still missing\n",
    "\n",
    "        # Final message based on installation success\n",
    "        if installation_success:\n",
    "            print(\"All libraries were installed successfully.\")\n",
    "        else:\n",
    "            print(\"Some libraries were not installed correctly.\")\n",
    "    else:\n",
    "        print(\"All libraries are already installed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_libraries(required_libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports <a name=\"library-imports\"></a>\n",
    "In this cell, we will import all the necessary libraries that we will use throughout the analysis. This includes libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading <a name=\"data-loading\"></a>\n",
    "Here, we will load the dataset containing information about students' alcohol consumption and related factors. We will examine the structure of the data and check for any initial issues such as missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the CSV files\n",
    "path_data_math = '../data/student-mat.csv'\n",
    "path_data_port = '../data/student-por.csv'\n",
    "\n",
    "# Reading the CSV files\n",
    "data_math = pd.read_csv(path_data_math)\n",
    "data_port = pd.read_csv(path_data_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematics Data:\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher  ...   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other  ...   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other  ...   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services  ...   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other  ...   \n",
      "\n",
      "  famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
      "0      4        3      4     1     1      3        6   5   6   6  \n",
      "1      5        3      3     1     1      3        4   5   5   6  \n",
      "2      4        3      2     2     3      3       10   7   8  10  \n",
      "3      3        2      2     1     1      5        2  15  14  15  \n",
      "4      4        3      2     1     2      5        4   6  10  10  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "Portuguese Data:\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher  ...   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other  ...   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other  ...   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services  ...   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other  ...   \n",
      "\n",
      "  famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
      "0      4        3      4     1     1      3        4   0  11  11  \n",
      "1      5        3      3     1     1      3        2   9  11  11  \n",
      "2      4        3      2     2     3      3        6  12  13  12  \n",
      "3      3        2      2     1     1      5        0  14  14  14  \n",
      "4      4        3      2     1     2      5        0  11  13  13  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of each dataset to verify the reading\n",
    "print(\"Mathematics Data:\")\n",
    "print(data_math.head())\n",
    "print(\"\\nPortuguese Data:\")\n",
    "print(data_port.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Combined Data: (382, 53)\n"
     ]
    }
   ],
   "source": [
    "# Concatenating the two DataFrames\n",
    "combined_data = pd.merge(data_math, data_port, on=[\"school\", \"sex\", \"age\", \"address\", \"famsize\", \n",
    "                             \"Pstatus\", \"Medu\", \"Fedu\", \"Mjob\", \"Fjob\", \n",
    "                             \"reason\", \"nursery\", \"internet\"])\n",
    "\n",
    "# Using shape to check the dimensions of the combined DataFrame\n",
    "print(\"\\nShape of Combined Data:\", combined_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration <a name=\"initial-data-exploration\"></a>\n",
    "In this section, we will perform an exploratory data analysis (EDA) to understand the dataset better. We will look at the data types, summary statistics, and any missing values, which will help us determine the next steps for cleaning and preparing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Each Column:\n",
      " school          0\n",
      "sex             0\n",
      "age             0\n",
      "address         0\n",
      "famsize         0\n",
      "Pstatus         0\n",
      "Medu            0\n",
      "Fedu            0\n",
      "Mjob            0\n",
      "Fjob            0\n",
      "reason          0\n",
      "guardian_x      0\n",
      "traveltime_x    0\n",
      "studytime_x     0\n",
      "failures_x      0\n",
      "schoolsup_x     0\n",
      "famsup_x        0\n",
      "paid_x          0\n",
      "activities_x    0\n",
      "nursery         0\n",
      "higher_x        0\n",
      "internet        0\n",
      "romantic_x      0\n",
      "famrel_x        0\n",
      "freetime_x      0\n",
      "goout_x         0\n",
      "Dalc_x          0\n",
      "Walc_x          0\n",
      "health_x        0\n",
      "absences_x      0\n",
      "G1_x            0\n",
      "G2_x            0\n",
      "G3_x            0\n",
      "guardian_y      0\n",
      "traveltime_y    0\n",
      "studytime_y     0\n",
      "failures_y      0\n",
      "schoolsup_y     0\n",
      "famsup_y        0\n",
      "paid_y          0\n",
      "activities_y    0\n",
      "higher_y        0\n",
      "romantic_y      0\n",
      "famrel_y        0\n",
      "freetime_y      0\n",
      "goout_y         0\n",
      "Dalc_y          0\n",
      "Walc_y          0\n",
      "health_y        0\n",
      "absences_y      0\n",
      "G1_y            0\n",
      "G2_y            0\n",
      "G3_y            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for missing values\n",
    "missing_values = combined_data.isnull().sum()\n",
    "print(\"Missing Values in Each Column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types of Each Column:\n",
      " school          object\n",
      "sex             object\n",
      "age              int64\n",
      "address         object\n",
      "famsize         object\n",
      "Pstatus         object\n",
      "Medu             int64\n",
      "Fedu             int64\n",
      "Mjob            object\n",
      "Fjob            object\n",
      "reason          object\n",
      "guardian_x      object\n",
      "traveltime_x     int64\n",
      "studytime_x      int64\n",
      "failures_x       int64\n",
      "schoolsup_x     object\n",
      "famsup_x        object\n",
      "paid_x          object\n",
      "activities_x    object\n",
      "nursery         object\n",
      "higher_x        object\n",
      "internet        object\n",
      "romantic_x      object\n",
      "famrel_x         int64\n",
      "freetime_x       int64\n",
      "goout_x          int64\n",
      "Dalc_x           int64\n",
      "Walc_x           int64\n",
      "health_x         int64\n",
      "absences_x       int64\n",
      "G1_x             int64\n",
      "G2_x             int64\n",
      "G3_x             int64\n",
      "guardian_y      object\n",
      "traveltime_y     int64\n",
      "studytime_y      int64\n",
      "failures_y       int64\n",
      "schoolsup_y     object\n",
      "famsup_y        object\n",
      "paid_y          object\n",
      "activities_y    object\n",
      "higher_y        object\n",
      "romantic_y      object\n",
      "famrel_y         int64\n",
      "freetime_y       int64\n",
      "goout_y          int64\n",
      "Dalc_y           int64\n",
      "Walc_y           int64\n",
      "health_y         int64\n",
      "absences_y       int64\n",
      "G1_y             int64\n",
      "G2_y             int64\n",
      "G3_y             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2. Check data types\n",
    "data_types = combined_data.dtypes\n",
    "print(\"\\nData Types of Each Column:\\n\", data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Data Types of Each Column:\n",
      "school          category\n",
      "sex             category\n",
      "age                int64\n",
      "address         category\n",
      "famsize         category\n",
      "Pstatus         category\n",
      "Medu            category\n",
      "Fedu            category\n",
      "Mjob            category\n",
      "Fjob            category\n",
      "reason          category\n",
      "guardian_x      category\n",
      "traveltime_x    category\n",
      "studytime_x     category\n",
      "failures_x      category\n",
      "schoolsup_x     category\n",
      "famsup_x        category\n",
      "paid_x          category\n",
      "activities_x    category\n",
      "nursery         category\n",
      "higher_x        category\n",
      "internet        category\n",
      "romantic_x      category\n",
      "famrel_x        category\n",
      "freetime_x      category\n",
      "goout_x         category\n",
      "Dalc_x          category\n",
      "Walc_x          category\n",
      "health_x        category\n",
      "absences_x         int64\n",
      "G1_x               int64\n",
      "G2_x               int64\n",
      "G3_x               int64\n",
      "guardian_y      category\n",
      "traveltime_y    category\n",
      "studytime_y     category\n",
      "failures_y      category\n",
      "schoolsup_y     category\n",
      "famsup_y        category\n",
      "paid_y          category\n",
      "activities_y    category\n",
      "higher_y        category\n",
      "romantic_y      category\n",
      "famrel_y        category\n",
      "freetime_y      category\n",
      "goout_y         category\n",
      "Dalc_y          category\n",
      "Walc_y          category\n",
      "health_y        category\n",
      "absences_y         int64\n",
      "G1_y               int64\n",
      "G2_y               int64\n",
      "G3_y               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3. Correcting Data Types for Categorical Variables\n",
    "\n",
    "# List the columns to convert to categorical\n",
    "columns_to_convert = [\n",
    "    'school', 'sex', 'address', 'famsize', 'Pstatus',\n",
    "    'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian_x',\n",
    "    'guardian_y', 'schoolsup_x', 'schoolsup_y', 'famsup_x',\n",
    "    'famsup_y', 'paid_x', 'paid_y', 'activities_x', 'activities_y', \n",
    "    'nursery', 'higher_x', 'higher_y', 'internet', 'romantic_x',\n",
    "    'romantic_y', 'traveltime_x', 'traveltime_y','studytime_x',\n",
    "    'studytime_y', 'failures_x', 'failures_y', 'famrel_x', 'famrel_y',\n",
    "    'freetime_x', 'freetime_y', 'goout_x', 'goout_y', 'Dalc_x', 'Dalc_y',\n",
    "    'Walc_x', 'Walc_y', 'health_x', 'health_y'\n",
    "]\n",
    "\n",
    "# Convert the specified columns to 'category' type\n",
    "combined_data[columns_to_convert] = combined_data[columns_to_convert].astype('category')\n",
    "\n",
    "# Check the data types after conversion\n",
    "print(\"Updated Data Types of Each Column:\")\n",
    "print(combined_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "               age    absences          G1          G2          G3\n",
      "count  790.000000  790.000000  790.000000  790.000000  790.000000\n",
      "mean    16.696203    5.708861   10.908861   10.713924   10.415190\n",
      "std      1.275234    7.998022    3.317091    3.759120    4.578538\n",
      "min     15.000000    0.000000    3.000000    0.000000    0.000000\n",
      "25%     16.000000    0.000000    8.000000    9.000000    8.000000\n",
      "50%     17.000000    4.000000   11.000000   11.000000   11.000000\n",
      "75%     18.000000    8.000000   13.000000   13.000000   14.000000\n",
      "max     22.000000   75.000000   19.000000   19.000000   20.000000\n"
     ]
    }
   ],
   "source": [
    "# 4. Summary statistics for numerical columns\n",
    "summary_statistics = combined_data.describe()\n",
    "print(\"\\nSummary Statistics:\\n\", summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Check unique values in categorical columns\n",
    "for column in combined_data.select_dtypes(include=['category']).columns:\n",
    "    print(f\"\\nUnique Values in '{column}':\\n\", combined_data[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize distributions for numerical columns using histograms\n",
    "combined_data.hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create boxplots to check for outliers in numerical columns\n",
    "for column in combined_data.select_dtypes(exclude=['category']).columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x=combined_data[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the outliers in the dataset, we have decided to retain them as they represent valid data points that are relevant for subsequent evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization <a name=\"data-visualization\"></a>\n",
    "Data visualization is key to understanding trends and patterns in our data. In this cell, we will create various plots to visualize the relationships between different variables, including alcohol consumption and academic performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis <a name=\"correlation-analysis\"></a>\n",
    "We will analyze the correlations between numerical features in the dataset. This will help us identify which factors are most strongly related to alcohol consumption and academic performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation <a name=\"data-preparation\"></a>\n",
    "Before modeling, we need to prepare the data. This includes handling missing values, encoding categorical variables, and splitting the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a name=\"modeling\"></a>\n",
    "In this section, we will build machine learning models to predict students' academic performance based on their alcohol consumption and other features. We will choose appropriate algorithms and fit them to our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation <a name=\"model-evaluation\"></a>\n",
    "After building our models, we will evaluate their performance using various metrics. We will compare the predictions against the actual outcomes to determine how well our models are performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps <a name=\"conclusion-and-next-steps\"></a>\n",
    "In the final section, we will summarize our findings from the analysis and discuss potential next steps. This may include further analysis, model improvements, or recommendations for educators based on the insights gained from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
